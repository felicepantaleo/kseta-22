{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Introduction to parallel programming course The course will take place on: Tuesday 22/02 from 9h00 to 17h00 Friday 25/02 from 9h00 to 17h00 Useful links: Discord: https://discord.gg/UKy2JtQz VPN: https://www.scc.kit.edu/en/services/vpn.php Jupyter: https://uc2-jupyter.scc.kit.edu/hub/home","title":"Welcome to the Introduction to parallel programming course"},{"location":"#welcome-to-the-introduction-to-parallel-programming-course","text":"The course will take place on: Tuesday 22/02 from 9h00 to 17h00 Friday 25/02 from 9h00 to 17h00","title":"Welcome to the Introduction to parallel programming course"},{"location":"#useful-links","text":"Discord: https://discord.gg/UKy2JtQz VPN: https://www.scc.kit.edu/en/services/vpn.php Jupyter: https://uc2-jupyter.scc.kit.edu/hub/home","title":"Useful links:"},{"location":"parallelism/1_parcpp/","text":"Environment Make sure you are using the correct environment! Hello World #include <thread> #include <iostream> int main() { auto f = [](int i){ std::cout << \"hello world from thread \" << i << std::endl; }; //Construct a thread which runs the function f std::thread t0(f,0); //and then destroy it by joining it t0.join(); } Compile with: g++ std_threads.cpp -lpthread -o std_threads Measuring time intervals #include <chrono> ... auto start = std::chrono::steady_clock::now(); foo(); auto stop = std::chrono::steady_clock::now(); std::chrono::duration<double> dur= stop - start; std::cout << dur.count() << \" seconds\" << std::endl; Exercise 1. Reduction #include <iostream> #include <random> #include <utility> #include <vector> #include <chrono> int main(){ const unsigned int numElements= 100000000; std::vector<int> input; input.reserve(numElements); std::mt19937 engine; std::uniform_int_distribution<> uniformDist(-5,5); for ( unsigned int i=0 ; i< numElements ; ++i) input.emplace_back(uniformDist(engine)); long long int sum= 0; auto f= [&](unsigned long long firstIndex, unsigned long long lastIndex){ for (auto it= firstIndex; it < lastIndex; ++it){ sum+= input[it]; } }; auto start = std::chrono::system_clock::now(); f(0,numElements); std::chrono::duration<double> dur= std::chrono::system_clock::now() - start; std::cout << \"Time spent in reduction: \" << dur.count() << \" seconds\" << std::endl; std::cout << \"Sum result: \" << sum << std::endl; return 0; } Quickly create threads unsigned int n = std::thread::hardware_concurrency(); std::vector<std::thread> v; for (int i = 0; i < n; ++i) { v.emplace_back(f,i); } for (auto& t : v) { t.join(); } Exercise 2. Numerical Integration #include <iostream> #include <iomanip> #include <chrono> int main() { double sum = 0.; constexpr unsigned int num_steps = 1 << 22; double pi = 0.0; constexpr double step = 1.0/(double) num_steps; auto start = std::chrono::system_clock::now(); for (int i=0; i< num_steps; i++){ auto x = (i+0.5)/num_steps; sum = sum + 4.0/(1.0+x*x); } auto stop = std::chrono::system_clock::now(); std::chrono::duration<double> dur= stop - start; std::cout << dur.count() << \" seconds\" << std::endl; pi = step * sum; std::cout << \"result: \" << std::setprecision (15) << pi << std::endl; } Exercise 3. pi with Montecarlo . The area of the circle is pi and the area of the square is 4. Generate N random floats x and y between -1 and 1 https://en.cppreference.com/w/cpp/numeric/random/uniform_real_distribution . Calculate the distance r of your point from the origin. If r < 1 : the point is inside the circle and increase Nin . The ratio between Nin and N converges to the ratio between the areas. Setting the environment for Intel oneTBB Check your environment! echo $TBBROOT To compile and link: g++ -O2 algo_par.cpp -ltbb Let's check that you can compile a simple tbb program: #include <cstdint> #include <oneapi/tbb.h> #include <oneapi/tbb/info.h> #include <oneapi/tbb/parallel_for.h> #include <oneapi/tbb/task_arena.h> #include <cassert> int main() { // Get the default number of threads int num_threads = oneapi::tbb::info::default_concurrency(); // Run the default parallelism oneapi::tbb::parallel_for( oneapi::tbb::blocked_range<size_t>(0, 20), [=](const oneapi::tbb::blocked_range<size_t> &r) { // Assert the maximum number of threads assert(num_threads == oneapi::tbb::this_task_arena::max_concurrency()); }); // Create the default task_arena oneapi::tbb::task_arena arena; arena.execute([=] { oneapi::tbb::parallel_for( oneapi::tbb::blocked_range<size_t>(0, 20), [=](const oneapi::tbb::blocked_range<size_t> &r) { // Assert the maximum number of threads assert(num_threads == oneapi::tbb::this_task_arena::max_concurrency()); }); }); return 0; } Compile with: g++ your_first_tbb_program.cpp -ltbb Your TBB Thread pool // analogous to hardware_concurrency, number of hw threads: int num_threads = oneapi::tbb::info::default_concurrency(); // or if you wish to force a number of threads: auto t = 10; //running with 10 threads oneapi::tbb::task_arena arena(t); // And query an arena for the number of threads used: auto max = oneapi::tbb::this_task_arena::max_concurrency(); // Limit the number of threads to two for all oneTBB parallel interfaces oneapi::tbb::global_control global_limit(oneapi::tbb::global_control::max_allowed_parallelism, 2); Task parallelism A task is submitted to a task_group as in the following. The run method is asynchronous. In order to be sure that the task has completed, the wait method has to be launched. Alternatively, the run_and_wait method can be used. #include <iostream> #include <oneapi/tbb.h> #include <oneapi/tbb/task_group.h> using namespace oneapi::tbb; int Fib(int n) { if (n < 2) { return n; } else { int x, y; task_group g; g.run([&] { x = Fib(n - 1); }); // spawn a task g.run([&] { y = Fib(n - 2); }); // spawn another task g.wait(); // wait for both tasks to complete return x + y; } } int main() { std::cout << Fib(32) << std::endl; return 0; } Bonus: Graph Traversal Generate a direct acyclic graph represented as a std::vector<Vertex> graph of 20 vertices: struct Vertex { int N; std::vector<int> Neighbors; } If there is a connection from A to B , the index of the element B in graph needs to be pushed into A.Neighbors . Make sure that from the first element of graph you can visit the entire graph. Once generated, when you visit a vertex X of the graph, you compute Fib(X.N) . Generate Vertex.N uniformly between 30 and 40. Remember to keep track of which vertex has already been visited.","title":"Parallel C++ and TBB"},{"location":"parallelism/1_parcpp/#environment","text":"Make sure you are using the correct environment!","title":"Environment"},{"location":"parallelism/1_parcpp/#hello-world","text":"#include <thread> #include <iostream> int main() { auto f = [](int i){ std::cout << \"hello world from thread \" << i << std::endl; }; //Construct a thread which runs the function f std::thread t0(f,0); //and then destroy it by joining it t0.join(); } Compile with: g++ std_threads.cpp -lpthread -o std_threads","title":"Hello World"},{"location":"parallelism/1_parcpp/#measuring-time-intervals","text":"#include <chrono> ... auto start = std::chrono::steady_clock::now(); foo(); auto stop = std::chrono::steady_clock::now(); std::chrono::duration<double> dur= stop - start; std::cout << dur.count() << \" seconds\" << std::endl;","title":"Measuring time intervals"},{"location":"parallelism/1_parcpp/#exercise-1-reduction","text":"#include <iostream> #include <random> #include <utility> #include <vector> #include <chrono> int main(){ const unsigned int numElements= 100000000; std::vector<int> input; input.reserve(numElements); std::mt19937 engine; std::uniform_int_distribution<> uniformDist(-5,5); for ( unsigned int i=0 ; i< numElements ; ++i) input.emplace_back(uniformDist(engine)); long long int sum= 0; auto f= [&](unsigned long long firstIndex, unsigned long long lastIndex){ for (auto it= firstIndex; it < lastIndex; ++it){ sum+= input[it]; } }; auto start = std::chrono::system_clock::now(); f(0,numElements); std::chrono::duration<double> dur= std::chrono::system_clock::now() - start; std::cout << \"Time spent in reduction: \" << dur.count() << \" seconds\" << std::endl; std::cout << \"Sum result: \" << sum << std::endl; return 0; }","title":"Exercise 1. Reduction"},{"location":"parallelism/1_parcpp/#quickly-create-threads","text":"unsigned int n = std::thread::hardware_concurrency(); std::vector<std::thread> v; for (int i = 0; i < n; ++i) { v.emplace_back(f,i); } for (auto& t : v) { t.join(); }","title":"Quickly create threads"},{"location":"parallelism/1_parcpp/#exercise-2-numerical-integration","text":"#include <iostream> #include <iomanip> #include <chrono> int main() { double sum = 0.; constexpr unsigned int num_steps = 1 << 22; double pi = 0.0; constexpr double step = 1.0/(double) num_steps; auto start = std::chrono::system_clock::now(); for (int i=0; i< num_steps; i++){ auto x = (i+0.5)/num_steps; sum = sum + 4.0/(1.0+x*x); } auto stop = std::chrono::system_clock::now(); std::chrono::duration<double> dur= stop - start; std::cout << dur.count() << \" seconds\" << std::endl; pi = step * sum; std::cout << \"result: \" << std::setprecision (15) << pi << std::endl; }","title":"Exercise 2. Numerical Integration"},{"location":"parallelism/1_parcpp/#exercise-3-pi-with-montecarlo","text":". The area of the circle is pi and the area of the square is 4. Generate N random floats x and y between -1 and 1 https://en.cppreference.com/w/cpp/numeric/random/uniform_real_distribution . Calculate the distance r of your point from the origin. If r < 1 : the point is inside the circle and increase Nin . The ratio between Nin and N converges to the ratio between the areas.","title":"Exercise 3. pi with Montecarlo"},{"location":"parallelism/1_parcpp/#setting-the-environment-for-intel-onetbb","text":"Check your environment! echo $TBBROOT To compile and link: g++ -O2 algo_par.cpp -ltbb Let's check that you can compile a simple tbb program: #include <cstdint> #include <oneapi/tbb.h> #include <oneapi/tbb/info.h> #include <oneapi/tbb/parallel_for.h> #include <oneapi/tbb/task_arena.h> #include <cassert> int main() { // Get the default number of threads int num_threads = oneapi::tbb::info::default_concurrency(); // Run the default parallelism oneapi::tbb::parallel_for( oneapi::tbb::blocked_range<size_t>(0, 20), [=](const oneapi::tbb::blocked_range<size_t> &r) { // Assert the maximum number of threads assert(num_threads == oneapi::tbb::this_task_arena::max_concurrency()); }); // Create the default task_arena oneapi::tbb::task_arena arena; arena.execute([=] { oneapi::tbb::parallel_for( oneapi::tbb::blocked_range<size_t>(0, 20), [=](const oneapi::tbb::blocked_range<size_t> &r) { // Assert the maximum number of threads assert(num_threads == oneapi::tbb::this_task_arena::max_concurrency()); }); }); return 0; } Compile with: g++ your_first_tbb_program.cpp -ltbb","title":"Setting the environment for Intel oneTBB"},{"location":"parallelism/1_parcpp/#your-tbb-thread-pool","text":"// analogous to hardware_concurrency, number of hw threads: int num_threads = oneapi::tbb::info::default_concurrency(); // or if you wish to force a number of threads: auto t = 10; //running with 10 threads oneapi::tbb::task_arena arena(t); // And query an arena for the number of threads used: auto max = oneapi::tbb::this_task_arena::max_concurrency(); // Limit the number of threads to two for all oneTBB parallel interfaces oneapi::tbb::global_control global_limit(oneapi::tbb::global_control::max_allowed_parallelism, 2);","title":"Your TBB Thread pool"},{"location":"parallelism/1_parcpp/#task-parallelism","text":"A task is submitted to a task_group as in the following. The run method is asynchronous. In order to be sure that the task has completed, the wait method has to be launched. Alternatively, the run_and_wait method can be used. #include <iostream> #include <oneapi/tbb.h> #include <oneapi/tbb/task_group.h> using namespace oneapi::tbb; int Fib(int n) { if (n < 2) { return n; } else { int x, y; task_group g; g.run([&] { x = Fib(n - 1); }); // spawn a task g.run([&] { y = Fib(n - 2); }); // spawn another task g.wait(); // wait for both tasks to complete return x + y; } } int main() { std::cout << Fib(32) << std::endl; return 0; }","title":"Task parallelism"},{"location":"parallelism/1_parcpp/#bonus-graph-traversal","text":"Generate a direct acyclic graph represented as a std::vector<Vertex> graph of 20 vertices: struct Vertex { int N; std::vector<int> Neighbors; } If there is a connection from A to B , the index of the element B in graph needs to be pushed into A.Neighbors . Make sure that from the first element of graph you can visit the entire graph. Once generated, when you visit a vertex X of the graph, you compute Fib(X.N) . Generate Vertex.N uniformly between 30 and 40. Remember to keep track of which vertex has already been visited.","title":"Bonus: Graph Traversal"},{"location":"parallelism/2_cuda/","text":"The CUDA Runtime API reference manual is a very useful source of information: http://docs.nvidia.com/cuda/cuda-runtime-api/index.html $ cd kseta-22/hands-on/cuda-exercises Check that your environment is correctly configured to compile CUDA code by running: $ nvcc --version Compile and run the deviceQuery application: cd kseta-22/hands-on/cuda-exercises/utils/deviceQuery make You can get some useful information about the features and the limits that you will find on the device you will be running your code on. For example: $ ./deviceQuery ./deviceQuery Starting... CUDA Device Query (Runtime API) version (CUDART static linking) Detected 1 CUDA Capable device(s) Device 0: \"Tesla V100-SXM2-32GB\" CUDA Driver Version / Runtime Version 11.4 / 11.4 CUDA Capability Major/Minor version number: 7.0 Total amount of global memory: 32510 MBytes (34089730048 bytes) (80) Multiprocessors, ( 64) CUDA Cores/MP: 5120 CUDA Cores GPU Max Clock rate: 1530 MHz (1.53 GHz) Memory Clock rate: 877 Mhz Memory Bus Width: 4096-bit L2 Cache Size: 6291456 bytes Maximum Texture Dimension Size (x,y,z) 1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384) Maximum Layered 1D Texture Size, (num) layers 1D=(32768), 2048 layers Maximum Layered 2D Texture Size, (num) layers 2D=(32768, 32768), 2048 layers Total amount of constant memory: 65536 bytes Total amount of shared memory per block: 49152 bytes Total shared memory per multiprocessor: 98304 bytes Total number of registers available per block: 65536 Warp size: 32 Maximum number of threads per multiprocessor: 2048 Maximum number of threads per block: 1024 Max dimension size of a thread block (x,y,z): (1024, 1024, 64) Max dimension size of a grid size (x,y,z): (2147483647, 65535, 65535) Maximum memory pitch: 2147483647 bytes Texture alignment: 512 bytes Concurrent copy and kernel execution: Yes with 6 copy engine(s) Run time limit on kernels: Yes Integrated GPU sharing Host Memory: No Support host page-locked memory mapping: Yes Alignment requirement for Surfaces: Yes Device has ECC support: Enabled Device supports Unified Addressing (UVA): Yes Device supports Managed Memory: Yes Device supports Compute Preemption: Yes Supports Cooperative Kernel Launch: Yes Supports MultiDevice Co-op Kernel Launch: Yes Device PCI Domain ID / Bus ID / location ID: 0 / 58 / 0 Compute Mode: < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) > deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 11.4, CUDA Runtime Version = 11.4, NumDevs = 1 Result = PASS Some of you are sharing the same machine and some time measurements can be influenced by other users running at the very same moment. It can be necessary to run time measurements multiple times. Exercise 1. CUDA Memory Model In this exercise you will learn what heterogeneous memory model means, by demonstrating the difference between host and device memory spaces. Allocate device memory; Copy the host array h_a to d_a on the device; Copy the device array d_a to the device array d_b; Copy the device array d_b to the host array h_a; Free the memory allocated for d_a and d_b. Compile and run the program by running: $ nvcc cuda_mem_model.cu -o ex01 $ ./ex01 Bonus: Measure the PCI Express bandwidth. Exercise 2. Launch a kernel By completing this exercise you will learn how to configure and launch a simple CUDA kernel. Allocate device memory; Configure the kernel to run using a one-dimensional grid of one-dimensional blocks; Each GPU thread should set one element of the array to: d_a[i] = blockIdx.x + threadIdx.x + 42; 4. Copy the results to the host memory; 5. Check the correctness of the results Exercise 3. Two-dimensional grid M is a matrix of NxN integers. Set N=4 Write a kernel that sets each element of the matrix to its linear index (e.g. M[2,3] = 2*N + 3), by making use of two-dimensional grid and blocks. (Two-dimensional means using the x and y coordinates). Copy the result to the host and check that it is correct. Try with a rectangular matrix 19x67. Hint: check the kernel launch parameters. Exercise 4. Measuring throughput The throughput of a kernel can be defined as the number of bytes read and written by a kernel in the unit of time. The CUDA event API includes calls to create and destroy events, record events, and compute the elapsed time in milliseconds between two recorded events. CUDA events make use of the concept of CUDA streams. A CUDA stream is simply a sequence of operations that are performed in order on the device. Operations in different streams can be interleaved and in some cases overlapped, a property that can be used to hide data transfers between the host and the device. Up to now, all operations on the GPU have occurred in the default stream, or stream 0 (also called the \"Null Stream\"). The peak theoretical throughput can be evaluated as well: if your device comes with a memory clock rate of 1GHz DDR (double data rate) and a 256-bit wide memory interface, the peak theoretical throughput can be computed with the following: Throughput (GB/s)= Memory_rate(Hz) * memory_interface_width(byte) * 2 /10 9 Compute the theoretical peak throughput of the device you are using. Modify ex04.cu to give the measurement of actual throughput of the kernel. Measure the throughput with a varying number of elements (in logarithmic scale). Before doing that write down what do you expect (you can also draw a diagram). What did you find out? Can you give an explanation? Exercise 5. Parallel Reduction Given an array a[N] , the reduction sum Sum of a is the sum of all its elements: Sum=a[0]+a[1]+...a[N-1] . 1. Implement a block-wise parallel reduction (using the shared memory). 2. For each block, save the partial sum. 3. Sum all the partial sums together. 4. Check the result comparing with the host result. 5. Measure the throughput of your reduction kernel using CUDA Events (see exercise 4) 6. Analyze your application using nvvp . Do you think it can be improved? How? * Bonus: Can you implement a one-step reduction? Measure and compare the throughput of the two versions. * Challenge: The cumulative sum of an array a[N] is another array b[N] , the sum of prefixes of a : b[i] = a[0] + a[1] + \u2026 + a[i] . Implement a cumulative sum kernel assuming that the size of the input array is multiple of the block size. Challenge: Histogram The purpose of this lab is to implement an efficient histogramming algorithm for an input array of integers within a given range. Each integer will map into a single bin, so the values will range from 0 to (NUM_BINS - 1). The histogram bins will use unsigned 32-bit counters that must be saturated at 127 (i.e. no roll back to 0 allowed). The input length can be assumed to be less than 2\u02c632. NUM_BINS is fixed at 4096 for this lab. This can be split into two kernels: one that does a histogram without saturation, and a final kernel that cleans up the bins if they are too large. These two stages can also be combined into a single kernel. Utility. Measuring time using CUDA Events cudaEvent_t start, stop; float time; cudaEventCreate(&start); cudaEventCreate(&stop); cudaEventRecord(start, 0); square_array <<< n_blocks, block_size >>> (a_d, N); cudaEventRecord(stop, 0); cudaEventSynchronize(stop); cudaEventElapsedTime(&time, start, stop); std::cout << \"Time for the kernel: \" << time << \" ms\" << std::endl; Atomics [1] An atomic function performs a read-modify-write atomic operation on one 32-bit or 64-bit word residing in global or shared memory. The operation is atomic in the sense that it is guaranteed to be performed without interference from other threads. int atomicAdd(int* address, int val); unsigned int atomicAdd(unsigned int* address, unsigned int val); unsigned long long int atomicAdd(unsigned long long int* address, unsigned long long int val); float atomicAdd(float* address, float val); double atomicAdd(double* address, double val); __half2 atomicAdd(__half2 *address, __half2 val); __half atomicAdd(__half *address, __half val); reads the 16-bit, 32-bit or 64-bit word old located at the address address in global or shared memory, computes (old + val), and stores the result back to memory at the same address. These three operations are performed in one atomic transaction. The function returns old.","title":"Introduction to CUDA"},{"location":"parallelism/2_cuda/#exercise-1-cuda-memory-model","text":"In this exercise you will learn what heterogeneous memory model means, by demonstrating the difference between host and device memory spaces. Allocate device memory; Copy the host array h_a to d_a on the device; Copy the device array d_a to the device array d_b; Copy the device array d_b to the host array h_a; Free the memory allocated for d_a and d_b. Compile and run the program by running: $ nvcc cuda_mem_model.cu -o ex01 $ ./ex01 Bonus: Measure the PCI Express bandwidth.","title":"Exercise 1. CUDA Memory Model"},{"location":"parallelism/2_cuda/#exercise-2-launch-a-kernel","text":"By completing this exercise you will learn how to configure and launch a simple CUDA kernel. Allocate device memory; Configure the kernel to run using a one-dimensional grid of one-dimensional blocks; Each GPU thread should set one element of the array to: d_a[i] = blockIdx.x + threadIdx.x + 42; 4. Copy the results to the host memory; 5. Check the correctness of the results","title":"Exercise 2. Launch a kernel"},{"location":"parallelism/2_cuda/#exercise-3-two-dimensional-grid","text":"M is a matrix of NxN integers. Set N=4 Write a kernel that sets each element of the matrix to its linear index (e.g. M[2,3] = 2*N + 3), by making use of two-dimensional grid and blocks. (Two-dimensional means using the x and y coordinates). Copy the result to the host and check that it is correct. Try with a rectangular matrix 19x67. Hint: check the kernel launch parameters.","title":"Exercise 3. Two-dimensional grid"},{"location":"parallelism/2_cuda/#exercise-4-measuring-throughput","text":"The throughput of a kernel can be defined as the number of bytes read and written by a kernel in the unit of time. The CUDA event API includes calls to create and destroy events, record events, and compute the elapsed time in milliseconds between two recorded events. CUDA events make use of the concept of CUDA streams. A CUDA stream is simply a sequence of operations that are performed in order on the device. Operations in different streams can be interleaved and in some cases overlapped, a property that can be used to hide data transfers between the host and the device. Up to now, all operations on the GPU have occurred in the default stream, or stream 0 (also called the \"Null Stream\"). The peak theoretical throughput can be evaluated as well: if your device comes with a memory clock rate of 1GHz DDR (double data rate) and a 256-bit wide memory interface, the peak theoretical throughput can be computed with the following: Throughput (GB/s)= Memory_rate(Hz) * memory_interface_width(byte) * 2 /10 9 Compute the theoretical peak throughput of the device you are using. Modify ex04.cu to give the measurement of actual throughput of the kernel. Measure the throughput with a varying number of elements (in logarithmic scale). Before doing that write down what do you expect (you can also draw a diagram). What did you find out? Can you give an explanation?","title":"Exercise 4. Measuring throughput"},{"location":"parallelism/2_cuda/#exercise-5-parallel-reduction","text":"Given an array a[N] , the reduction sum Sum of a is the sum of all its elements: Sum=a[0]+a[1]+...a[N-1] . 1. Implement a block-wise parallel reduction (using the shared memory). 2. For each block, save the partial sum. 3. Sum all the partial sums together. 4. Check the result comparing with the host result. 5. Measure the throughput of your reduction kernel using CUDA Events (see exercise 4) 6. Analyze your application using nvvp . Do you think it can be improved? How? * Bonus: Can you implement a one-step reduction? Measure and compare the throughput of the two versions. * Challenge: The cumulative sum of an array a[N] is another array b[N] , the sum of prefixes of a : b[i] = a[0] + a[1] + \u2026 + a[i] . Implement a cumulative sum kernel assuming that the size of the input array is multiple of the block size.","title":"Exercise 5. Parallel Reduction"},{"location":"parallelism/2_cuda/#challenge-histogram","text":"The purpose of this lab is to implement an efficient histogramming algorithm for an input array of integers within a given range. Each integer will map into a single bin, so the values will range from 0 to (NUM_BINS - 1). The histogram bins will use unsigned 32-bit counters that must be saturated at 127 (i.e. no roll back to 0 allowed). The input length can be assumed to be less than 2\u02c632. NUM_BINS is fixed at 4096 for this lab. This can be split into two kernels: one that does a histogram without saturation, and a final kernel that cleans up the bins if they are too large. These two stages can also be combined into a single kernel.","title":"Challenge: Histogram"},{"location":"parallelism/2_cuda/#utility-measuring-time-using-cuda-events","text":"cudaEvent_t start, stop; float time; cudaEventCreate(&start); cudaEventCreate(&stop); cudaEventRecord(start, 0); square_array <<< n_blocks, block_size >>> (a_d, N); cudaEventRecord(stop, 0); cudaEventSynchronize(stop); cudaEventElapsedTime(&time, start, stop); std::cout << \"Time for the kernel: \" << time << \" ms\" << std::endl;","title":"Utility. Measuring time using CUDA Events"},{"location":"parallelism/2_cuda/#atomics-1","text":"An atomic function performs a read-modify-write atomic operation on one 32-bit or 64-bit word residing in global or shared memory. The operation is atomic in the sense that it is guaranteed to be performed without interference from other threads. int atomicAdd(int* address, int val); unsigned int atomicAdd(unsigned int* address, unsigned int val); unsigned long long int atomicAdd(unsigned long long int* address, unsigned long long int val); float atomicAdd(float* address, float val); double atomicAdd(double* address, double val); __half2 atomicAdd(__half2 *address, __half2 val); __half atomicAdd(__half *address, __half val); reads the 16-bit, 32-bit or 64-bit word old located at the address address in global or shared memory, computes (old + val), and stores the result back to memory at the same address. These three operations are performed in one atomic transaction. The function returns old.","title":"Atomics [1]"},{"location":"setup/Setup/","text":"How to connect to JupyterHub The KIT VPN is required to connect to the remote cluster. Information to use the KIT VPN can be found here https://www.scc.kit.edu/en/services/vpn.php . Under Remote Access (VPN) , select your operating system and follow the instructions With the VPN enabled, ssh <username>@uc2.scc.kit.edu . This is needed to trigger the creation of your home folder. Visit https://uc2-jupyter.scc.kit.edu/hub/home , select your organization and login with your credentials. You should see the following page Click on \"Start My Server\", you will be redirected to Configure the server as follows: Note to select gpu_4 you have to click on Advanced Mode first. Remember to write kseta in Reservation, this gives you priority in the queue. Click on Spawn and wait the server launch! Configure Jupyter Notebook Once you are logged in you should be able to see the following interface: Before doing anything , load the following modules: compiler/gnu/9.3 devel/tbb/2021.4.0 devel/cuda/11.4 Now you can open a shell and download the repository for the exercises! git clone https://github.com/felicepantaleo/kseta-22 Enjoy the hacking!","title":"Setup"},{"location":"setup/Setup/#how-to-connect-to-jupyterhub","text":"The KIT VPN is required to connect to the remote cluster. Information to use the KIT VPN can be found here https://www.scc.kit.edu/en/services/vpn.php . Under Remote Access (VPN) , select your operating system and follow the instructions With the VPN enabled, ssh <username>@uc2.scc.kit.edu . This is needed to trigger the creation of your home folder. Visit https://uc2-jupyter.scc.kit.edu/hub/home , select your organization and login with your credentials. You should see the following page Click on \"Start My Server\", you will be redirected to Configure the server as follows: Note to select gpu_4 you have to click on Advanced Mode first. Remember to write kseta in Reservation, this gives you priority in the queue. Click on Spawn and wait the server launch!","title":"How to connect to JupyterHub"},{"location":"setup/Setup/#configure-jupyter-notebook","text":"Once you are logged in you should be able to see the following interface: Before doing anything , load the following modules: compiler/gnu/9.3 devel/tbb/2021.4.0 devel/cuda/11.4 Now you can open a shell and download the repository for the exercises! git clone https://github.com/felicepantaleo/kseta-22 Enjoy the hacking!","title":"Configure Jupyter Notebook"}]}